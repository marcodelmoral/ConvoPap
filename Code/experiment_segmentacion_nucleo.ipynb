{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import display\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from utils import PlotLosses\n",
    "\n",
    "ruta = 'C:/Users/marco/Documents/ConvCervix'\n",
    "directorio_experimento = f'{ruta}/segmentacion_nucleo'\n",
    "if not os.path.exists(directorio_experimento):\n",
    "    os.mkdir(directorio_experimento)\n",
    "os.chdir(directorio_experimento)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    # first layer\n",
    "    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    # second layer\n",
    "    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\")(x)\n",
    "    if batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def unet(input_img, n_filters=16, dropout=0.5, batchnorm=True, optimizer='adam', loss='binary-crossentropy', metrics='accuracy'):\n",
    "    # contracting path\n",
    "    inputTensor = Input(input_img)\n",
    "    c1 = conv2d_block(inputTensor, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2)) (c1)\n",
    "    p1 = tf.keras.layers.Dropout(dropout*0.5)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2)) (c2)\n",
    "    p2 = tf.keras.layers.Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2)) (c3)\n",
    "    p3 = tf.keras.layers.Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    p4 = tf.keras.layers.Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    # expansive path\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = tf.keras.layers.Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    u7 = tf.keras.layers.Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    u8 = tf.keras.layers.Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    u9 = tf.keras.layers.Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[inputTensor], outputs=[outputs])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return 0.5 * tf.keras.losses.binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n",
    "\n",
    "def carga_datos(df, x, y):\n",
    "    for i, row in df.iterrows():\n",
    "        x[i] = cv2.imread(row['file'])\n",
    "        mascara = cv2.imread(row['file_mask'], cv2.IMREAD_GRAYSCALE)\n",
    "        mascara = mascara[:, :, np.newaxis]\n",
    "        mascara = mascara/255\n",
    "        y[i] = mascara\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "WIDTH = 256 # 256\n",
    "HEIGTH = 256 # 256\n",
    "LR = 1e-4\n",
    "OPT = tf.keras.optimizers.Adam(lr=LR)\n",
    "SEED = 111091\n",
    "DROPOUT = 0.5\n",
    "np.random.seed(SEED)\n",
    "EPOCHS = 50\n",
    "FILTERS = 16\n",
    "BATCHNORM = True\n",
    "LOSS_FUNC = bce_dice_loss\n",
    "METRICS = [tf.keras.metrics.MeanIoU(num_classes=2), 'accuracy']\n",
    "KFOLD_SPLITS = 10\n",
    "INPUT_FORM = (WIDTH, HEIGTH, 3)\n",
    "EVALUACIONES = []\n",
    "NUM_CLASSES = 2\n",
    "avg_acc = []\n",
    "avg_loss = []\n",
    "avg_io = []\n",
    "kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "datos = pd.read_csv(f'{ruta}/database/augment_database/database_augment.csv')\n",
    "\n",
    "datos_random = datos.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "HYP = {'Valor': [BATCH_SIZE, WIDTH, HEIGTH, LR, OPT, SEED, EPOCHS, DROPOUT, BATCHNORM, KFOLD_SPLITS, NUM_CLASSES]}\n",
    "df_hyp = pd.DataFrame(HYP, \n",
    "                      index=['BATCH_SIZE', 'WIDTH', 'HEIGTH', 'LR', 'OPT', 'SEED', 'EPOCHS', 'DROPOUT', 'BATCHNORM','KFOLD_SPLITS',' NUM_CLASSES'])\n",
    "display(df_hyp)\n",
    "df_hyp.to_csv('hiperparametros.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_indices, val_indices) in enumerate(kf.split(datos_random)):\n",
    "    fold_dir = f'fold_{i}'\n",
    "    print(f'Iniciando Fold: {i}')\n",
    "    if not os.path.exists(fold_dir):\n",
    "        os.mkdir(fold_dir)\n",
    "\n",
    "    print(f'Datos de entrenamiento: {len(train_indices)}')\n",
    "    print(f'Datos de validacion: {len(val_indices)}')\n",
    "    \n",
    "    print('Dividiendo datos')\n",
    "    train = datos.iloc[train_indices]\n",
    "    val = datos.iloc[val_indices]\n",
    "    train.to_csv(os.path.join(fold_dir, \"datos_train.csv\"))\n",
    "    val.to_csv(os.path.join(fold_dir, \"datos_val.csv\"))\n",
    "    \n",
    "    xtrain = np.ndarray(shape=(len(train.index), WIDTH, HEIGHT, 3), dtype=np.float16)\n",
    "    ytrain = np.ndarray(shape=(len(train.index), WIDTH, HEIGHT, 1), dtype=np.float16)\n",
    "    \n",
    "    xval = np.ndarray(shape=(len(val.index), WIDTH, HEIGHT, 3), dtype=np.float16)\n",
    "    yval = np.ndarray(shape=(len(val.index), WIDTH, HEIGHT, 1), dtype=np.float16)\n",
    "    \n",
    "    xtrain, ytrain = carga_datos(train, xtrain, ytrain)\n",
    "    \n",
    "    xval, yval = carga_datos(val, xval, yval)\n",
    "\n",
    "    print('Construyendo modelo')\n",
    "    model = unet(INPUT_FORM, \n",
    "             n_filters=FILTERS, \n",
    "             dropout=DROPOUT, \n",
    "             batchnorm=BATCHNORM, \n",
    "             optimizer=OPT, \n",
    "             loss=LOSS_FUNC, \n",
    "             metrics=METRICS)\n",
    "    \n",
    "    print('Creando generadores')\n",
    "    data_gen_args = dict(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='constant')\n",
    "    X_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
    "    Y_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
    "    X_train_augmented = X_datagen.flow(xtrain, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)\n",
    "    Y_train_augmented = Y_datagen.flow(ytrain, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)\n",
    "     \n",
    "    X_datagen_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    Y_datagen_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    X_test_augmented = X_datagen_val.flow(xval, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)\n",
    "    Y_test_augmented = Y_datagen_val.flow(yval, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)\n",
    "    \n",
    "    # combinar generadores\n",
    "    train_generator = zip(X_train_augmented, Y_train_augmented)\n",
    "    test_generator = zip(X_test_augmented, Y_test_augmented)\n",
    "    \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"{fold_dir}/model.best.h5\", \n",
    "                                                 monitor=\"val_mean_iou\", \n",
    "                                                 verbose=1, \n",
    "                                                 mode='max', \n",
    "                                                 save_best_only=True)\n",
    "    \n",
    "    plot_losses = PlotLosses(figsize=(10,6))\n",
    "    callbacks_list = [\n",
    "                      plot_losses, \n",
    "                      checkpoint, \n",
    "                      tf.keras.callbacks.CSVLogger(os.path.join(fold_dir, 'log.csv'))\n",
    "    ]\n",
    "    \n",
    "    with open('summary.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "            \n",
    "    tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    "    )\n",
    "    \n",
    "    print('Iniciando entrenamiento')\n",
    "    history = model.fit(train_generator, \n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=math.ceil(len(val_indices) / BATCH_SIZE), # batch_size/2\n",
    "                    steps_per_epoch=math.ceil(len(train_indices) / BATCH_SIZE),  # len(x)/(batch_size*2)\n",
    "                    epochs=EPOCKS, \n",
    "                    callbacks=callbacks_list, \n",
    "                    verbose=1, \n",
    "                    shuffle=True)\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "    mean_iou = history.history['mean_iou']\n",
    "    val_mean_iou = history.history['val_mean_iou']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    avg_loss.append(val_loss[-1])\n",
    "    avg_acc.append(val_accuracy[-1]*100)\n",
    "    avg_mean_iou.append(val_mean_iou[-1]*100)\n",
    "    \n",
    "    print('Salvando modelo')       \n",
    "    model.save(os.path.join(fold_dir, 'model.h5'))\n",
    "\n",
    "print(f'Resultados de la validación cruzada (K = {KFOLD_SPLITS})')\n",
    "print(f'Pérdida {np.mean(avg_loss)} (+/- {np.std(avg_loss)}%)')\n",
    "print(f'Precisión {np.mean(avg_acc)}% (+/- {np.std(avg_acc)}%)')\n",
    "print(f'IOU promedio {np.mean(avg_mean_iou)}% (+/- {np.std(avg_mean_iou)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendimiento = dict(acc=avg_acc, loss=avg_loss, mean_iou=avg_mean_iou)\n",
    "df_rendimiento = pd.DataFrame(rendimiento, index=list(range(0,KFOLD_SPLITS)))\n",
    "df_rendimiento.index.name = 'fold'\n",
    "df_rendimiento.to_csv('rendimiento_experimento.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}